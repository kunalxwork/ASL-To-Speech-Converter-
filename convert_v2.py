# -*- coding: utf-8 -*-
"""convert_v2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MgbZt6h14xOnLW--FfWxLu-8DOBBYHoo
"""

import tensorflow as tf
import numpy as np
import os

# --- !! TRY THIS FIRST !! ---
# Set this to False to try 8-bit quantization first.
# If it fails again, set it to True to try 16-bit float quantization.
USE_FLOAT16_QUANTIZATION = False
# -----------------------------

# --- Define Your File Paths ---
CSV_PATH = 'model/keypoint_classifier/keypoint.csv'
KERAS_MODEL_PATH = 'model/keypoint_classifier/keypoint_classifier.keras'
SAVED_MODEL_DIR = 'model/keypoint_classifier/saved_model'
TFLITE_PATH = 'model/keypoint_classifier/keypoint_classifier.tflite'
NUM_SAMPLES_FOR_GEN = 500

def representative_data_gen():
    print(f"Loading {NUM_SAMPLES_FOR_GEN} samples from {CSV_PATH} for representative dataset...")
    try:
        X_data_sample = np.loadtxt(
            CSV_PATH,
            delimiter=',',
            dtype='float32',
            usecols=list(range(1, (21 * 2) + 1)),
            max_rows=NUM_SAMPLES_FOR_GEN
        )
    except Exception as e:
        print(f"Error loading CSV file: {e}")
        return

    print("Dataset loaded. Starting generator...")
    for input_value in X_data_sample:
        yield [np.array([input_value], dtype=np.float32)]

def convert_model():
    # --- 1. Load the Keras Model ---
    if not os.path.exists(KERAS_MODEL_PATH):
        print(f"Error: Keras model file not found at {KERAS_MODEL_PATH}")
        return

    print(f"Loading Keras model from {KERAS_MODEL_PATH}...")
    model = tf.keras.models.load_model(KERAS_MODEL_PATH)
    print("Keras model loaded successfully.")

    # --- 2. Export as SavedModel format (THE FIX) ---
    # This is the new command for Keras 3
    print(f"Exporting to SavedModel format at {SAVED_MODEL_DIR}...")
    model.export(SAVED_MODEL_DIR)
    print("SavedModel format exported.")

    # --- 3. Set up the Converter from the SavedModel ---
    converter = tf.lite.TFLiteConverter.from_saved_model(SAVED_MODEL_DIR)

    if USE_FLOAT16_QUANTIZATION:
        # --- Option A: Float16 Quantization ---
        print("Using Float16 quantization...")
        converter.optimizations = [tf.lite.Optimize.DEFAULT]
        converter.target_spec.supported_types = [tf.float16]
    else:
        # --- Option B: 8-bit Integer Quantization ---
        print("Using 8-bit Integer quantization with representative dataset...")
        converter.optimizations = [tf.lite.Optimize.DEFAULT]
        converter.representative_dataset = representative_data_gen
        converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
        converter.inference_input_type = tf.int8
        converter.inference_output_type = tf.int8

    # --- 4. Run the Conversion ---
    print("Starting TFLite conversion... This may take a few minutes.")
    try:
        tflite_model = converter.convert()
    except Exception as e:
        print(f"\n--- CONVERSION FAILED ---")
        print(f"Error: {e}")
        print("Try setting USE_FLOAT16_QUANTIZATION = True in the script and run again.")
        return

    # --- 5. Save the TFLite File ---
    with open(TFLITE_PATH, 'wb') as f:
        f.write(tflite_model)

    print(f"\n--- SUCCESS! ---")
    print(f"Quantized TFLite model saved to: {TFLITE_PATH}")

if __name__ == '__main__':
    convert_model()